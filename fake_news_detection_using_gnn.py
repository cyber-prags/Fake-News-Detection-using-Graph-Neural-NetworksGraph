# -*- coding: utf-8 -*-
"""Fake News Detection using GNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UKRvDVe2Xd38E7BGkGj0ZxmmpRxtHStd

## Installing the necessary utilities
"""

# # PyG installation
# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html
# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html
# !pip install -q git+https://github.com/rusty1s/pytorch_geometric.git

!pip install networkx

"""## Importing necessary libraries"""

import torch
import torch_geometric
import pandas as pd
import numpy as np
import networkx as nx

vers = torch.__version__
print("Torch version: ",vers)

"""## Importing our dataset

### Dataset

- Contains news propagation graphs extracted from Twitter
- Source and raw data: https://github.com/KaiDMML/FakeNewsNet
- Preprocessing: https://arxiv.org/pdf/2104.12259.pdf
- feature="content" --> Spacy Word2Vec + Profile features
"""

from torch_geometric.datasets import UPFD

train_data = UPFD(root='data/UPFD',name = "gossipcop", feature="content", split='train')
test_data = UPFD(root='data/UPFD',name = "gossipcop", feature="content", split='test')

print("Size of training dataset:" ,len(train_data))
print("Size of test dataset:" ,len(test_data))

train_data[0]

"""## Exploratory Data analysis"""

sample_id=1
train_data[sample_id].edge_index

def to_networkx(data, node_attrs=None, edge_attrs=None, to_undirected=False, remove_self_loops=False):
    """
    Converts a PyTorch Geometric Data object to a NetworkX graph.

    Args:
        data (torch_geometric.data.Data): A graph object from PyTorch Geometric.
        node_attrs (list, optional): List of node attribute keys to include in the graph. Defaults to None.
        edge_attrs (list, optional): List of edge attribute keys to include in the graph. Defaults to None.
        to_undirected (bool, optional): If True, converts the graph to an undirected graph. Defaults to False.
        remove_self_loops (bool, optional): If True, removes self-loops from the graph. Defaults to False.

    Returns:
        networkx.Graph or networkx.DiGraph: A NetworkX graph.
    """
    # Create an undirected or directed graph
    G = nx.Graph() if to_undirected else nx.DiGraph()

    # Add nodes to the graph
    G.add_nodes_from(range(data.num_nodes))

    # Initialize attributes
    node_attrs = node_attrs or []
    edge_attrs = edge_attrs or []
    values = {}

    # Extract node and edge attributes from the data object
    for key, item in data(*(node_attrs + edge_attrs)):
        if torch.is_tensor(item):
            values[key] = item.squeeze().tolist()  # Convert tensor to a list
        else:
            values[key] = item
        # Handle single-element lists or tuples
        if isinstance(values[key], (list, tuple)) and len(values[key]) == 1:
            values[key] = values[key][0]

    # Add edges to the graph
    for i, (u, v) in enumerate(data.edge_index.t().tolist()):
        if to_undirected and v > u:
            continue  # Avoid duplicate edges in undirected graphs
        if remove_self_loops and u == v:
            continue  # Skip self-loops if specified
        G.add_edge(u, v)
        # Add edge attributes
        for key in edge_attrs:
            G[u][v][key] = values[key][i]

    # Add node attributes
    for key in node_attrs:
        for i, feat_dict in G.nodes(data=True):
            feat_dict.update({key: values[key][i]})

    return G

nx.draw(to_networkx(train_data[sample_id]))

nx.draw(to_networkx(test_data[sample_id]))

print(train_data[sample_id].x.shape)
train_data[sample_id].x



"""### Class Distribution"""

labels = [data.y.item() for i, data in enumerate(train_data)]
data = pd.DataFrame(labels, columns=['labels'])
data["labels"].hist()

"""It is generally observed that such datasets have a huge class imbalance; however in this case, it doesn't seem to be a problem."""



"""### Data Loader"""

from torch_geometric.loader import DataLoader

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

"""## Model & Training

From our evaluation above, we see that our graph is directed.

Therefore, it will only share information from the root.
"""

from torch_geometric.nn import global_max_pool as gmp
from torch_geometric.nn import GATConv
from torch.nn import Linear

from torch_geometric.nn import global_max_pool as gmp
from torch_geometric.nn import GATConv
from torch.nn import Linear


class GNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()

        # Graph Convolutions
        self.conv1 = GATConv(in_channels, hidden_channels)
        self.conv2 = GATConv(hidden_channels, hidden_channels)
        self.conv3 = GATConv(hidden_channels, hidden_channels)

        # Readout
        self.lin_news = Linear(in_channels, hidden_channels)
        self.lin0 = Linear(hidden_channels, hidden_channels)
        self.lin1 = Linear(2*hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        # Graph Convolutions
        h = self.conv1(x, edge_index).relu()
        h = self.conv2(h, edge_index).relu()
        h = self.conv3(h, edge_index).relu()

        # Pooling
        h = gmp(h, batch)

        # Readout
        h = self.lin0(h).relu()

        # According to UPFD paper: Include raw word2vec embeddings of news
        # This is done per graph in the batch
        root = (batch[1:] - batch[:-1]).nonzero(as_tuple=False).view(-1)
        root = torch.cat([root.new_zeros(1), root + 1], dim=0)
        # root is e.g. [   0,   14,   94,  171,  230,  302, ... ]
        news = x[root]
        news = self.lin_news(news).relu()

        out = self.lin1(torch.cat([h, news], dim=-1))
        return torch.sigmoid(out)

GNN(train_data.num_features, 128, 1)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, roc_curve
)
import pandas as pd
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GNN(train_data.num_features, 128, 1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)
loss_fnc = torch.nn.BCELoss()

# Training function
def train(epoch):
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
        loss.backward()
        optimizer.step()
        total_loss += float(loss) * data.num_graphs
    return total_loss / len(train_loader.dataset)

# Testing function
@torch.no_grad()
def test(epoch):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    for data in test_loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
        total_loss += float(loss) * data.num_graphs
        all_preds.append(torch.reshape(out, (-1,)))
        all_labels.append(data.y.float())

    # Calculate Metrics
    acc, f1, precision, recall, roc_auc = metrics(all_preds, all_labels)

    return total_loss / len(test_loader.dataset), acc, f1, precision, recall, roc_auc, all_preds, all_labels


# Metric calculation function
def metrics(preds, gts):
    preds = torch.round(torch.cat(preds)).cpu()
    gts = torch.cat(gts).cpu()
    acc = accuracy_score(gts, preds)
    f1 = f1_score(gts, preds)
    precision = precision_score(gts, preds)
    recall = recall_score(gts, preds)
    roc_auc = roc_auc_score(gts, preds)
    return acc, f1, precision, recall, roc_auc


# Confusion Matrix plot
def plot_confusion_matrix(preds, gts):
    preds = torch.round(torch.cat(preds)).cpu()
    gts = torch.cat(gts).cpu()
    cm = confusion_matrix(gts, preds)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()


# ROC Curve plot
def plot_roc_curve(preds, gts):
    preds = torch.cat(preds).cpu().numpy()
    gts = torch.cat(gts).cpu().numpy()
    fpr, tpr, _ = roc_curve(gts, preds)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label='ROC Curve')
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

# Training loop with metrics and logging
train_losses, test_losses = [], []
accuracies, f1s, precisions, recalls, roc_aucs = [], [], [], [], []

for epoch in range(1, 41):  # 40 epochs
    train_loss = train(epoch)
    test_loss, acc, f1, precision, recall, roc_auc, all_preds, all_labels = test(epoch)

    # Logging
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    accuracies.append(acc)
    f1s.append(f1)
    precisions.append(precision)
    recalls.append(recall)
    roc_aucs.append(roc_auc)

    print(f'Epoch: {epoch:02d} | TrainLoss: {train_loss:.4f} | TestLoss: {test_loss:.4f} | '
          f'Accuracy: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}')

# Plot training and test loss
plt.figure(figsize=(10, 6))
plt.plot(range(1, 41), train_losses, label='Train Loss')
plt.plot(range(1, 41), test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss')
plt.legend()
plt.show()

# Plot metrics
plt.figure(figsize=(10, 6))
plt.plot(range(1, 41), accuracies, label='Accuracy')
plt.plot(range(1, 41), f1s, label='F1 Score')
plt.plot(range(1, 41), precisions, label='Precision')
plt.plot(range(1, 41), recalls, label='Recall')
plt.plot(range(1, 41), roc_aucs, label='ROC-AUC')
plt.xlabel('Epoch')
plt.ylabel('Metric Value')
plt.title('Metrics Over Epochs')
plt.legend()
plt.show()

# Evaluate final predictions
plot_confusion_matrix(all_preds, all_labels)
plot_roc_curve(all_preds, all_labels)

# Display prediction dataframe
for data in test_loader:
    data = data.to(device)
    pred = model(data.x, data.edge_index, data.batch)
    df = pd.DataFrame()
    df["pred_logit"] = pred.detach().cpu().numpy()[:, 0]
    df["pred"] = torch.round(pred).detach().cpu().numpy()[:, 0]
    df["true"] = data.y.cpu().numpy()
    print(df.head(10))
    break

